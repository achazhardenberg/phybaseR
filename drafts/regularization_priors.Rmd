## Example 4: Handling Multicollinearity

Sometimes your predictors are highly correlated (multicollinearity), causing the model to crash or produce wildly uncertain estimates with "exploding" standard errors.

In these specific numerical cases, **Regularization** is necessary to get an answer at all. We can use a stronger Normal prior (Ridge-like) to stabilize the variance.

> [!CAUTION]
> **Do NOT use strong priors to "fix" your results.**
>
> It is tempting to use a strong prior to shrink a "pesky" coefficient towards zero so that your d-separation test passes. **This is bad science.**
>
> *   **Rule of Thumb**: Only use regularization when explicitly diagnosing a numerical issue (e.g., Variance Inflation Factor > 10, or effectively singular matrix).
> *   **Transparency**: If you use a strong prior, you **must** report it and ideally show the sensitivity of your results by comparing it to the Default model.

```{r ridge_regression}
# Simulate Multicollinearity
set.seed(99)
N <- 15
# Two highly correlated predictors: r > 0.99
Temp_Max <- rnorm(N, 20, 5)
Temp_Min <- Temp_Max - 10 + rnorm(N, sd = 0.5)

# Both affect Growth
Growth <- 0.5 * Temp_Max + 0.3 * Temp_Min + rnorm(N, sd = 3)
df_collinear <- data.frame(Temp_Max, Temp_Min, Growth)

# 1. Fit Default Model (Weak Priors)
# With high collinearity, the parameter space is a flat "ridge",
# and the model cannot easily distinguish the two effects.
fit_weak <- because(
    equations = list(Growth ~ Temp_Max + Temp_Min),
    data = df_collinear,
    n.iter = 1000,
    quiet = TRUE
)

# 2. Fit Stabilized Model (Ridge Priors)
# Precision = 5.0 acts as a "soft constraint" preventing explosion
ridge_priors <- list(
    beta_Growth_Temp_Max = "dnorm(0, 5.0)",
    beta_Growth_Temp_Min = "dnorm(0, 5.0)"
)

fit_ridge <- because(
    equations = list(Growth ~ Temp_Max + Temp_Min),
    data = df_collinear,
    priors = ridge_priors,
    n.iter = 1000,
    quiet = TRUE
)

# Visualize: Ridge reduces the uncertainty (variance) caused by collinearity
plot_posterior(
    list(Unstable = fit_weak, Stabilized = fit_ridge),
    parameter = "beta_Growth_Temp_Max"
)
```

### Reflection: Structural vs. Statistical Solutions

A keen user might ask: **"Why not fix collinearity structurally using SEM?"**

You are absolutely right. In a full Structural Equation Model, we could treat `Temp_Max` and `Temp_Min` as indicators of a single Latent Variable (e.g., `Temperature`).
*   **Structural Solution**: `Temperature -> Growth`. This removes collinearity by collapsing the variables.
*   **Statistical Solution (Ridge)**: Keep `Temp_Max` and `Temp_Min` but shrink their variances.

**Which to use?**
*   If `Temp_Max` and `Temp_Min` represent the *same underlying concept*, the Structural (Latent) solution is better science.
*   If they represent *distinct mechanisms* you must test separately (but happen to be correlated), Ridge is the necessary statistical tool to stabilize the estimation.

**Conclusion**: Use Ridge when you *must* keep collinear predictors separate. Use Latent Variables (or PCA) if they are redundant measurements of one thing. And **never** use either just to force a p-value.

---
title: "phybaseR: An R package to easily create and run PhyBaSE models"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{phybaseR: An R package to easily create and run PhyBaSE models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Introduction

**phybaseR** is an R package designed to easily perform causal inference in phylogenetic comparative analyses implementing and extending the methods proposed in von Hardenberg & Gonzalez-Voyer (2025). It allows you to fit Phylogenetic Bayesian Structural Equation Models (SEMs) that account for:

*   **Phylogenetic Non-independence**: Using the phylogenetic covariance matrix.
*   **Measurement Error**: Incorporating standard errors or repeated measures.
*   **Missing Data**: Automatically handling missing values in both predictors and responses.
*   **Phylogenetic Uncertainty**: Integrating over a posterior sample of trees.
*   **Multiple Response Distributions**: 
    - Gaussian (continuous data)
    - Binomial (binary/proportion data)
    - Multinomial (unordered categorical data)
    - Ordinal (ordered categorical data)
    - Poisson (count data)
    - Negative Binomial (overdispersed count data)
*   **Categorical Predictors**: Automatic handling of factor variables with dummy coding.
*   **Optimized Performance**: Fast random effects formulation (4.6× speedup) for large datasets.

This tutorial will guide you through the main features of the package.


## Setup

First, load the package and the necessary dependencies.

```{r setup}
library(phybaseR)
library(ape)
```

**Note on Tree Standardization**: `phybaseR` automatically standardizes phylogenetic trees to have a maximum branching time of 1.0. This improves numerical stability and convergence. You'll see a message like "Standardizing tree edge lengths (max branching time: 1.27 -> 1.0)" when this occurs. This doesn't affect your results—it's just internal scaling.

## Basic Usage: Phylogenetic Regression (PGLS)

Let's start with a simple example: a phylogenetic regression of `Y` on `X`. We'll simulate some data for demonstration.

```{r basic_sim}
set.seed(123)
N <- 50
tree <- rtree(N)

# Simulate traits
X <- rTraitCont(tree, model = "BM", sigma = 1)
Y <- 0.5 + 0.8 * X + rTraitCont(tree, model = "BM", sigma = 0.5)

data_list <- list(X = X, Y = Y)
```

To run the model, we define the structural equations and pass them to `phybase_run()`.

```{r basic_run, eval=FALSE}
# Define equations
equations <- list(Y ~ X)

# Run model
fit <- phybase_run(
    data = data_list,
    tree = tree,
    equations = equations,
    n.iter = 5000,
    n.burnin = 1000,
    n.chains = 3
)

# View summary
summary(fit)
```

## Mediation Analysis

PhyBaSE shines at testing causal paths, such as mediation: `X -> M -> Y`.

```{r mediation_sim}
# Simulate mediator M
M <- 0.3 + 0.6 * X + rTraitCont(tree, model = "BM", sigma = 0.5)
# Simulate Y affected by both X and M
Y_med <- 0.5 + 0.4 * X + 0.5 * M + rTraitCont(tree, model = "BM", sigma = 0.5)

data_med <- list(X = X, M = M, Y = Y_med)
```

```{r mediation_run, eval=FALSE}
# Define DAG equations
equations_med <- list(
    M ~ X,
    Y ~ X + M
)

fit_med <- phybase_run(
    data = data_med,
    tree = tree,
    equations = equations_med
)

summary(fit_med)
```

## Measurement Error

PhyBaSE can account for measurement error in your variables.

### Option 1: Using Standard Errors (SE)

If you have known standard errors for your species means:

```{r me_se, eval=FALSE}
# Assume we have SEs for X
X_se <- rep(0.1, N)
data_se <- list(X = X, Y = Y, X_se = X_se)

fit_se <- phybase_run(
    data = data_se,
    tree = tree,
    equations = list(Y ~ X),
    variability = c(X = "se") # Tell PhyBaSE that X has SEs
)

summary(fit_se)
```

**Note on DIC with measurement error**: When using measurement error (either SE or repeated measures), the DIC penalty will be inflated because PhyBaSE creates latent "true" values for each species. The penalty will be approximately: structural parameters + N (number of species). This is expected behavior.

- For **model comparison**, use WAIC instead: `phybase_run(..., WAIC = TRUE)`
- For **fit quality**, focus on the mean deviance (lower is better)
- The inflated penalty does not affect the validity of the model, only the DIC interpretation

### Option 2: Using Repeated Measures

If you have raw data (multiple observations per species), PhyBaSE can estimate the observation error directly.

```{r me_reps, eval=FALSE}
# Simulate unequal repeated measures (ragged array)
# Some species have 2 reps, some have 3
max_reps <- 3
X_obs <- matrix(NA, nrow = N, ncol = max_reps)

for (i in 1:N) {
    # Randomly decide if species has 2 or 3 reps
    n_i <- sample(2:3, 1)

    # Simulate data
    vals <- rnorm(n_i, mean = X[i], sd = 0.2)

    # Fill matrix (padding with NA for missing reps)
    X_obs[i, 1:n_i] <- vals
}

fit_reps <- phybase_run(
    data = list(X = X_obs, Y = Y),
    tree = tree,
    equations = list(Y ~ X),
    variability = c(X = "reps")
)

summary(fit_reps)

# PhyBaSE automatically handles the NAs and counts valid replicates per species.
```
m
## Binomial Variables

You can model binary traits (binomial) using a phylogenetic logistic regression framework.

```{r binomial, eval=FALSE}
# Simulate binary trait
prob <- 1 / (1 + exp(-(0.5 + 0.8 * X)))
BinaryTrait <- rbinom(N, 1, prob)

data_bin <- list(X = X, Bin = BinaryTrait)

fit_bin <- phybase_run(
    data = data_bin,
    tree = tree,
    equations = list(Bin ~ X),
    distribution = c(Bin = "binomial") # Specify distribution
)

summary(fit_bin)
```

**Note**: Binomial variables should generally be child nodes (responses) in your DAG.

## Multinomial Variables

For categorical traits with more than two unordered levels (e.g., diet type: Carnivore, Herbivore, Omnivore), you can use the multinomial distribution.

```{r multinomial, eval=FALSE}
# Simulate multinomial trait (3 categories)
# Latent variables for k=2, 3 (k=1 is reference)
L2 <- -1 + 0.5 * X
L3 <- 1 - 0.5 * X
P1 <- 1 / (1 + exp(L2) + exp(L3))
P2 <- exp(L2) / (1 + exp(L2) + exp(L3))
P3 <- exp(L3) / (1 + exp(L2) + exp(L3))

MultiTrait <- apply(cbind(P1, P2, P3), 1, function(p) sample(1:3, 1, prob = p))

data_multi <- list(X = X, Multi = MultiTrait)

fit_multi <- phybase_run(
    data = data_multi,
    tree = tree,
    equations = list(Multi ~ X),
    distribution = c(Multi = "multinomial")
)

summary(fit_multi)
```

**Note**: The summary will show parameters for each category $k \ge 2$ (e.g., `beta_Multi_X[2]`, `beta_Multi_X[3]`), relative to the reference category $k=1$.

## Ordinal Variables

For **ordered categorical data** (e.g., IUCN Red List categories: LC < NT < VU < EN < CR), use the ordinal distribution. This respects the ordering of categories and is more appropriate than multinomial for such data.

```{r ordinal, eval=FALSE}
# Simulate IUCN-like conservation status with phylogenetic signal
# DAG: BodyMass -> IUCN <- HabitatLoss
#      BodyMass -> HabitatLoss
# Implied independence: IUCN ⊥ BodyMass | HabitatLoss
library(MASS)

# Phylogenetically structured predictors
VCV <- ape::vcv.phylo(tree)

# Body Mass (exogenous)
lambda_mass <- 0.6
Sigma_mass <- lambda_mass * VCV + (1 - lambda_mass) * diag(N)
BodyMass <- as.vector(mvrnorm(1, mu = rep(0, N), Sigma = Sigma_mass))

# Habitat Loss (caused by body mass)
lambda_habitat <- 0.5
tau_u_habitat <- 1 / 0.4
tau_e_habitat <- 1 / 0.3
u_std_habitat <- as.vector(mvrnorm(1, mu = rep(0, N), Sigma = solve(VCV)))
HabitatLoss <- 1.2 + 0.7 * BodyMass + u_std_habitat / sqrt(tau_u_habitat) +
    rnorm(N, 0, sqrt(1 / tau_e_habitat))

# IUCN status (ordinal response, caused by both predictors)
lambda_IUCN <- 0.75
tau_u_IUCN <- 1 / 0.5
tau_e_IUCN <- 1 / 0.3
u_std_IUCN <- as.vector(mvrnorm(1, mu = rep(0, N), Sigma = solve(VCV)))
phylo_error <- u_std_IUCN / sqrt(tau_u_IUCN) + rnorm(N, 0, sqrt(1 / tau_e_IUCN))

# Linear predictor: larger mass + more habitat loss -> higher threat
eta <- -0.5 * BodyMass + 0.8 * HabitatLoss + phylo_error
cutpoints <- c(-1, 0, 1, 2) # Thresholds between categories

# Calculate probabilities using cumulative logit
Q <- matrix(0, nrow = N, ncol = 4)
for (i in 1:N) {
    for (k in 1:4) {
        Q[i, k] <- 1 / (1 + exp(-(cutpoints[k] - eta[i])))
    }
}

# Category probabilities
P <- matrix(0, nrow = N, ncol = 5)
P[, 1] <- Q[, 1]
for (k in 2:4) {
    P[, k] <- Q[, k] - Q[, k - 1]
}
P[, 5] <- 1 - Q[, 4]

# Sample ordinal outcome (1=LC, 2=NT, 3=VU, 4=EN, 5=CR)
IUCN <- apply(P, 1, function(p) sample(1:5, 1, prob = p))

data_ordinal <- list(
    BodyMass = BodyMass,
    HabitatLoss = HabitatLoss,
    IUCN = IUCN,
    K_IUCN = 5
)

# Fit the DAG
fit_ordinal <- phybase_run(
    data = data_ordinal,
    tree = tree,
    equations = list(
        IUCN ~ BodyMass + HabitatLoss,
        HabitatLoss ~ BodyMass
    ),
    distribution = c(IUCN = "ordinal"),
    optimise = TRUE
)

summary(fit_ordinal)

# Check recovery of phylogenetic signal
fit_ordinal$summary$statistics["lambdaIUCN", ]

# Test d-separation: IUCN ⊥ BodyMass | HabitatLoss
fit_dsep <- phybase_run(
    data = data_ordinal,
    tree = tree,
    equations = list(
        IUCN ~ BodyMass + HabitatLoss,
        HabitatLoss ~ BodyMass
    ),
    distribution = c(IUCN = "ordinal"),
    dsep = TRUE,
    optimise = TRUE
)

# Check if betaBodyMass is close to zero in the d-sep test
summary(fit_dsep$samples)
# If 95% CI of betaBodyMass includes zero -> independence supported
```


**Key advantages of ordinal over multinomial**:
- Respects category ordering (statistical validity)
- More parsimonious (fewer parameters)
- Easier interpretation (single $\beta$ coefficient, not $K-1$)

**Note**: You'll see estimated cutpoints in the output (`cutpoint_IUCN[1]`, `cutpoint_IUCN[2]`, etc.), which define the thresholds between adjacent categories.

## Poisson Variables (Count Data)

For **count data** (e.g., number of species, offspring count, population size), use the Poisson distribution. The random effects formulation naturally handles **overdispersion** via the residual variance component.

```{r poisson, eval=FALSE}
# Simulate count data with phylogenetic signal (e.g., species richness)
# DAG: Temperature -> Richness <- Area
#      Temperature -> Area
# Implied independence: Richness ⊥ Temperature | Area
library(MASS)

# Phylogenetically structured predictors
VCV <- ape::vcv.phylo(tree)

# Temperature (exogenous)
lambda_temp <- 0.5
Sigma_temp <- lambda_temp * VCV + (1 - lambda_temp) * diag(N)
Temperature <- as.vector(mvrnorm(1, mu = rep(0, N), Sigma = Sigma_temp))

# Area (caused by temperature)
lambda_area <- 0.6
tau_u_area <- 1 / 0.4
tau_e_area <- 1 / 0.3
u_std_area <- as.vector(mvrnorm(1, mu = rep(0, N), Sigma = solve(VCV)))
Area <- 2 + 0.6 * Temperature + u_std_area / sqrt(tau_u_area) +
    rnorm(N, 0, sqrt(1 / tau_e_area))

# Phylogenetically structured error term for richness
lambda_richness <- 0.7 # True phylogenetic signal in Richness
tau_u <- 1 / 0.5 # Phylogenetic variance
tau_e <- 1 / 0.3 # Residual variance
u_std <- as.vector(mvrnorm(1, mu = rep(0, N), Sigma = solve(VCV)))
u <- u_std / sqrt(tau_u)
epsilon <- rnorm(N, 0, sqrt(1 / tau_e))
phylo_error <- u + epsilon

# True model: log(E[Richness]) = 2 + 0.5*Area + phylo_error
# Temperature affects richness ONLY through Area
log_richness <- 2 + 0.5 * Area + phylo_error
richness <- rpois(N, lambda = exp(log_richness))

data_poisson <- list(
    Temperature = Temperature,
    Area = Area,
    Richness = richness
)

# Fit the DAG
fit_poisson <- phybase_run(
    data = data_poisson,
    tree = tree,
    equations = list(
        Richness ~ Area,
        Area ~ Temperature
    ),
    distribution = c(Richness = "poisson"),
    optimise = TRUE
)

summary(fit_poisson)

# Check recovery of phylogenetic signal
# lambdaRichness should be ~ 0.7
fit_poisson$summary$statistics["lambdaRichness", ]

# Test d-separation: Richness ⊥ Temperature | Area
fit_dsep <- phybase_run(
    data = data_poisson,
    tree = tree,
    equations = list(
        Richness ~ Area,
        Area ~ Temperature
    ),
    distribution = c(Richness = "poisson"),
    dsep = TRUE,
    optimise = TRUE
)

# Check if betaTemperature is close to zero in the d-sep test
summary(fit_dsep$samples)
# If 95% CI of betaTemperature includes zero -> independence supported
```

**Overdispersion handling**: Unlike standard Poisson GLMs, the random effects formulation includes `tau_e` (residual precision), which absorbs extra-Poisson variation. This makes it equivalent to a **quasi-Poisson** model without needing an explicit overdispersion parameter.

**When to use Poisson vs. Negative Binomial**:
- **Poisson**: Default for count data; handles moderate overdispersion
- **Negative Binomial**: Heavy overdispersion (variance >> mean)

## Negative Binomial Variables (Overdispersed Counts)

For **heavily overdispersed count data** where variance far exceeds the mean, use the Negative Binomial distribution. It includes an explicit **size parameter** (`r`) that controls overdispersion.

```{r negbinomial, eval=FALSE}
# Simulate heavily overdispersed count data with phylogenetic signal
# DAG: HostSize -> Parasites <- Stress
#      HostSize -> Stress
# Implied independence: Parasites ⊥ HostSize | Stress
library(MASS)

# Phylogenetically structured predictors
VCV <- ape::vcv.phylo(tree)

# Host Size (exogenous)
lambda_size <- 0.4
Sigma_size <- lambda_size * VCV + (1 - lambda_size) * diag(N)
HostSize <- as.vector(mvrnorm(1, mu = rep(0, N), Sigma = Sigma_size))

# Stress (caused by host size)
lambda_stress <- 0.5
tau_u_stress <- 1 / 0.4
tau_e_stress <- 1 / 0.3
u_std_stress <- as.vector(mvrnorm(1, mu = rep(0, N), Sigma = solve(VCV)))
Stress <- 1.5 + 0.6 * HostSize + u_std_stress / sqrt(tau_u_stress) +
    rnorm(N, 0, sqrt(1 / tau_e_stress))

# Phylogenetically structured error term for parasites
lambda_parasites <- 0.8 # True phylogenetic signal
tau_u <- 1 / 0.6
tau_e <- 1 / 0.4
u_std <- as.vector(mvrnorm(1, mu = rep(0, N), Sigma = solve(VCV)))
u <- u_std / sqrt(tau_u)
epsilon <- rnorm(N, 0, sqrt(1 / tau_e))
phylo_error <- u + epsilon

# True model with overdispersion
# Stress increases parasite load (HostSize affects load ONLY through Stress)
r_true <- 2 # Size parameter (smaller = more overdispersed)
log_mu <- 2 + 0.7 * Stress + phylo_error
mu <- exp(log_mu)
parasite_count <- rnbinom(N, size = r_true, mu = mu)

data_nb <- list(
    HostSize = HostSize,
    Stress = Stress,
    Parasites = parasite_count
)

# Fit the DAG
fit_nb <- phybase_run(
    data = data_nb,
    tree = tree,
    equations = list(
        Parasites ~ Stress,
        Stress ~ HostSize
    ),
    distribution = c(Parasites = "negbinomial"),
    optimise = TRUE
)

summary(fit_nb)

# Check recovery of phylogenetic signal
# lambdaParasites should be ~ 0.8
fit_nb$summary$statistics["lambdaParasites", ]

# Test d-separation: Parasites ⊥ HostSize | Stress
fit_dsep <- phybase_run(
    data = data_nb,
    tree = tree,
    equations = list(
        Parasites ~ Stress,
        Stress ~ HostSize
    ),
    distribution = c(Parasites = "negbinomial"),
    dsep = TRUE,
    optimise = TRUE
)

# Check if betaHostSize is close to zero in the d-sep test
summary(fit_dsep$samples)
# If 95% CI of betaHostSize includes zero -> independence supported
```

**Understanding the size parameter (`r`)**:
- **Smaller `r`** → More overdispersion (variance >> mean)
- **Larger `r`** → Less overdispersion (approaches Poisson)
- **Typical values**: r = 0.5-5 for heavily overdispersed biological data

```
**Poisson vs. Negative Binomial decision tree**:
1. **Variance ≈ Mean**: Use Poisson (simpler, fewer parameters)
2. **Variance > Mean** (2-5x): Poisson with random effects (handles it via `tau_e`)
3. **Variance >> Mean** (>5x): Negative Binomial (explicit overdispersion modeling)

## Categorical Predictors

`phybaseR` automatically handles categorical predictors (factor or character variables) by converting them to **dummy variables** and **auto-expanding** equations.

```{r categorical, eval=FALSE}
# Data with categorical predictor
data <- data.frame(
    Species = tree$tip.label,
    BodyMass = rnorm(N),
    Diet = sample(c("Carnivore", "Herbivore", "Omnivore"), N, replace = TRUE),
    Habitat = factor(sample(c("Forest", "Grassland"), N, replace = TRUE))
)

# phybase_format_data automatically creates dummy variables
data_list <- phybase_format_data(data, tree = tree)
# Messages:
# Categorical variable 'Diet' expanded to 2 dummy variable(s) | Reference: 'Carnivore'
# Categorical variable 'Habitat' expanded to 1 dummy variable(s) | Reference: 'Forest'

# Use categorical variables DIRECTLY in equations - auto-expands!
fit <- phybase_run(
    data = data_list,
    tree = tree,
    equations = list(BodyMass ~ Diet + Habitat) # Auto-expands to dummies!
)
# Messages:
# Expanded 'Diet' to: Diet_Herbivore, Diet_Omnivore
# Expanded 'Habitat' to: Habitat_Grassland

summary(fit)
```

**Key points**:
- **Auto-expansion**: Write `Y ~ Diet`, get `Y ~ Diet_Herbivore + Diet_Omnivore`
- **Reference category**: First level alphabetically (e.g., "Carnivore" for Diet)
- **K-1 encoding**: K levels → K-1 dummy variables
- **Interpretation**: Coefficients are effects relative to reference category

**Example interpretation**:
- `beta_BodyMass_Diet_Herbivore = -0.65` → Herbivores 0.65 units lighter than Carnivores
- `beta_BodyMass_Habitat_Grassland = 0.56` → Grassland species 0.56 units heavier than Forest

## Missing Data

PhyBaSE fully supports missing data (`NA`) in both response and predictor variables. It uses a **Latent Variable (GLMM)** approach to impute missing values while preserving phylogenetic signal.

```{r missing, eval=FALSE}
# Introduce missing values
X_miss <- X
X_miss[c(1, 5, 10)] <- NA # Missing in predictor
Y_miss <- Y
Y_miss[c(2, 6, 12)] <- NA # Missing in response

data_miss <- list(X = X_miss, Y = Y_miss)

# Just run it! No extra setup needed.
fit_miss <- phybase_run(
    data = data_miss,
    tree = tree,
    equations = list(Y ~ X)
)

summary(fit_miss)

# PhyBaSE automatically detects NAs and handles them.
```

## Phylogenetic Uncertainty

To account for uncertainty in the phylogeny itself, simply pass a list of trees (e.g., a `multiPhylo` object) instead of a single tree.

```{r multitree, eval=FALSE}
# For demonstration, simulate multiple trees
num_trees <- 20
trees <- lapply(1:num_trees, function(x) rtree(N))

fit_multi <- phybase_run(
    data = data_list,
    tree = trees, # Pass list of trees
    equations = equations
)

summary(fit_multi)
```

## Parallel Execution

Running MCMC chains in parallel can significantly speed up your analysis, especially for complex models or when using many iterations.

### Parallel Chains

To run chains in parallel, simply set `parallel = TRUE` and specify the number of cores with `n.cores`.

```{r parallel, eval=FALSE}
fit_parallel <- phybase_run(
    data = data_list,
    tree = tree,
    equations = equations,
    n.iter = 10000,
    n.chains = 4,
    parallel = TRUE,
    n.cores = 4 # Use 4 cores for 4 chains
)
```

**Note**: When running in parallel, DIC and WAIC calculation requires a brief model recompilation step (enabled by default with `ic_recompile = TRUE`). This ensures you still get model comparison metrics.

### Parallel Model Comparison

If you have a list of models to compare, you can run them all in parallel using `phybase_compare()`.

```{r parallel_compare, eval=FALSE}
# Define multiple models
models <- list(
    model1 = list(equations = list(Y ~ X)),
    model2 = list(equations = list(Y ~ 1)) # Null model
)

# Run all models in parallel
results <- phybase_compare(
    models = models,
    data = data_list,
    tree = tree,
    n.iter = 5000,
    n.cores = 2 # Run 2 models at a time
)

# Compare WAIC
print(results$comparison)
```

## Model Comparison (WAIC)

You can compare models using the Widely Applicable Information Criterion (WAIC). There are two ways to calculate WAIC:

### Option 1: Calculate during model fitting

Set `WAIC = TRUE` in `phybase_run()` to calculate WAIC automatically:

```{r waic_during_run, eval=FALSE}
fit <- phybase_run(
    data = data,
    tree = tree,
    equations = equations,
    WAIC = TRUE, # Calculate WAIC during fitting
    n.chains = 2 # WAIC requires at least 2 chains
)

# WAIC is stored in the fit object
fit$WAIC
```

### Option 2: Calculate from an existing fitted model

If you didn't calculate WAIC initially, use the standalone `phybase_waic()` function. This is useful because you don't need to rerun the entire model - it uses the existing MCMC samples:

```{r waic_standalone, eval=FALSE}
# Calculate WAIC from a previously fitted model
waic_res <- phybase_waic(fit, n.iter = 2000)
waic_res
```

**Note**: WAIC requires at least 2 chains. If you only ran 1 chain, you'll need to refit the model with `n.chains = 2` or more before calculating WAIC.


## Model Validation (d-separation)

You can validate your model structure by testing the conditional independence statements implied by your DAG. This is done using the d-separation (basis set) method.

```{r dsep_example, eval=FALSE}
# Define a path model: A -> B -> C
# This implies: A is independent of C, conditional on B
A <- rTraitCont(tree)
B <- 0.5 + 0.8 * A + rTraitCont(tree)
C <- 0.5 + 0.8 * B + rTraitCont(tree)

data_dsep <- list(A = A, B = B, C = C)

equations_dsep <- list(
    B ~ A,
    C ~ B
)

# Run d-separation tests by setting dsep = TRUE
fit_dsep <- phybase_run(
    data = data_dsep,
    tree = tree,
    equations = equations_dsep,
    dsep = TRUE # <--- This triggers d-sep testing
)

# The summary will show the conditional independence tests
fit_dsep <- phybase_run(
    data = data_dsep,
    tree = tree,
    equations = equations_dsep,
    dsep = TRUE
)

summary(fit_dsep)
```

### Note on Sequential Testing

In the original PhyBaSE methodology (von Hardenberg & Gonzalez-Voyer, 2025), d-separation tests were proposed to be run within a single JAGS model by renaming variables (e.g., `BM`, `BM2`) to avoid cyclic dependencies in the Directed Acyclic Graph (DAG). 

`phybaseR` automates this process by running each d-separation test **sequentially** as a separate JAGS model. This approach offers several advantages for automation:

1.  **Computational Robustness**: It completely avoids cyclic dependency errors (e.g., "Unable to resolve parameter") that can occur in JAGS when multiple conditional independence tests imply conflicting directions of causality (e.g., testing $X \perp Y | Z$ and $Y \perp X | W$ simultaneously).
2.  **Statistical Validity**: D-separation tests are tests of *local* conditional independence. Running them sequentially is statistically valid because each test evaluates a specific implication of the DAG using the observed data.
3.  **Local Imputation**: For datasets with missing values, this approach implies that imputation is performed "locally" for each test based on the variables included in that specific test's conditioning set (plus the phylogeny). This is a standard and valid approach for piecewise SEM, ensuring that each test correctly propagates uncertainty given the local model structure.


**Output:**
```
PhyBaSE d-separation Tests
==========================

           Test Parameter Estimate LowerCI UpperCI Indep P_approx_0
 C _||_ A | {B}     betaA   -0.090  -0.724   0.518   Yes      0.776

Joint P(all tests ≈ 0) = 0.776

Legend:
  Indep: 'Yes' = Conditionally Independent, 'No' = Dependent (based on 95% CI)
  P(≈0): Bayesian probability that effect crosses zero (0-1 scale)
  Joint: Probability that ALL tests simultaneously support independence

Note: For d-separation, we expect high P(≈0) and Joint values (close to 1).
```

The `P(≈0)` gives you a Bayesian measure of how plausible independence is for each individual test, while the `Joint` probability tells you how often ALL tests simultaneously support the model structure.

## Latent Variables

PhyBaSE can handle **unobserved (latent) variables** using two different approaches:

### Approach 1: MAG (Correlations) - **Default**

The **Maximal Ancestral Graph (MAG)** approach marginalizes latent variables and models the **induced correlations** they create between observed variables.

```{r latent_mag, eval=FALSE}
# Suppose we have a latent variable L that affects both X and Y
# L -> X, L -> Y (L is unmeasured)

equations <- list(
    X ~ L,
    Y ~ L
)

fit_mag <- phybase_run(
    data = data_list,
    tree = tree,
    equations = equations,
    latent = "L",
    latent_method = "correlations", # Default, can omit
    n.iter = 10000
)

# Check induced correlations
fit_mag$induced_correlations
# [[1]]
# [1] "X" "Y"

# The correlation parameter rho_X_Y is estimated
summary(fit_mag)
```

**How it works**:
1. Converts your DAG to a MAG
2. Identifies bidirected edges `X <-> Y` from latent common causes
3. Models correlations using correlated residuals in JAGS

**Advantages**: Consistent with Shipley's d-separation framework, no need to worry about latent variable identification.

### Approach 2: Explicit Latent Variables

Alternatively, you can model latents as **actual JAGS nodes** and estimate the structural paths from latents to observed variables.

```{r latent_explicit, eval=FALSE}
fit_explicit <- phybase_run(
    data = data_list,
    tree = tree,
    equations = list(X ~ L, Y ~ L),
    latent = "L",
    latent_method = "explicit", # Model L as a node
    n.iter = 10000
)

# Estimates betaX_L and betaY_L
summary(fit_explicit)
```

**Advantages**: More flexible, allows estimation of latent -> observed paths.

**Disadvantages**: Requires careful consideration of identification and scaling.

**Note**: `phybaseR` automatically standardizes latent variables (fixes variance to 1) when using the explicit approach. This ensures identification and makes path coefficients interpretable as standardized effects. This is consistent with the recommendation to standardize all variables before running PhyBaSE models.

### Understanding Identification Issues

When using the **explicit approach**, latent variables have no inherent scale, leading to **identification problems** where multiple parameter combinations fit the data equally well.

**Example**: Consider `L -> X` and `L -> Y`:
- You could double the scale of `L` and halve both path coefficients → **same predictions**
- JAGS may struggle to converge or produce unreliable estimates

**Why MAG avoids this**: The MAG approach only estimates the **correlation** `rho_X_Y` induced by `L`, which is scale-invariant. You don't need to worry about the scale of `L` or its individual paths.

**How phybaseR handles this**: When using `latent_method = "explicit"`, `phybaseR` automatically **standardizes latent variables** by fixing their variance to 1 (`tau_L <- 1` in the JAGS model). This:
- Ensures the model is identified (no scale ambiguity)
- Makes path coefficients interpretable as standardized effects
- Is consistent with the recommendation to standardize all variables

**Example**: With `L -> X` and `L -> Y`:
- `L` has variance = 1 (standardized)
- `betaX_L = 0.8` means "1 SD change in L → 0.8 SD change in X"
- Path coefficients are directly comparable

### Recommendations

- **Default to MAG (correlations)**: Safer, more interpretable, consistent with d-separation testing
- **Use explicit sparingly**: Only when you have good reasons and understand the identification constraints

### Limitations

- MAG approach currently supports **pairwise correlations** only
- Explicit approach requires constraints for identification with complex latent structures



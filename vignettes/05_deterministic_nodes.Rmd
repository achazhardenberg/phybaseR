---
title: "05. Deterministic Nodes: Interactions, Logic, and Math"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Deterministic Nodes}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
    collapse = TRUE,
    comment = "#>"
)
```

# The Physics of Causal Models

In standard Bayesian regression, variables are typically modeled as **Stochastic Nodes**:
$$Y_i \sim \text{Normal}(\alpha + \beta X_i, \sigma)$$
This means $Y$ is *correlated* with $X$, but has its own independent "noise".

However, in many causal models, some variables are **logical consequences** of others.
For example:
*   **Interactions**: `Risk = Exposure * Toxicity`
*   **Thresholds**: `IsAdult = (Age > 2)`
*   **Transformations**: `LogSize = log(Size)`

If you treat `IsAdult` as a stochastic node (e.g., using a logistic regression on Age), the model might learn the correlation, but it treats the relationship as "soft".
Crucially, if `Age` is missing (NA), the model might impute `Age = 5` but `IsAdult = 0`. This violates the laws of physics (or logic).

`because` solves this with **Deterministic Nodes**.
These are variables defined by a hard equation (`<-`), ensuring they always stay perfectly synchronized with their parents, even during imputation and counterfactual interventions.

## Setup

```{r setup}
library(because)
set.seed(123)
```

## Example 1: Interactions (`A * B`)

The classic example. Usually, you have to create a column `AxB` in your data.
But if `A` or `B` is missing, `AxB` becomes NA, and standard models don't know that `AxB` depends on `A`.

With `because`, you just write the formula naturally:

```{r ex1}
N <- 100
tree <- ape::rtree(N)

# Predictors
Rain <- rnorm(N)
Temp <- rnorm(N)

# Interaction: Growth depends on Rain * Temp
Growth <- 0.5 * (Rain * Temp) + rnorm(N, sd = 0.1)

data <- list(Growth = Growth, Rain = Rain, Temp = Temp)

# Note the formula: Growth ~ Rain * Temp
fit_int <- because(
    equations = list(
        Growth ~ Rain * Temp
    ),
    data = data,
    tree = tree,
    n.iter = 1000,
    quiet = TRUE
)

# The model creates a deterministic node for the interaction
# Notice the parameter name "beta_Growth_Rain_x_Temp"
print(head(fit_int$summary$statistics))
```

## Example 2: Logic and Thresholds (`A > 2`)

You can model "tipping points" or categorical definitions.
For example, an animal is only "Mature" if it is older than 2 years.

```{r ex2}
# Predictors
Age <- runif(N, 0, 10)

# Deterministic threshold: 1 if Age > 5, else 0
IsMature <- as.numeric(Age > 5)

# Outcome: Mating success depends on Maturity
Mating <- 2 * IsMature + rnorm(N, sd = 0.5)

list_logic <- list(Mating = Mating, Age = Age)

# Use I() to wrap the logic
fit_logic <- because(
    equations = list(
        Mating ~ I(Age > 5)
    ),
    data = list_logic,
    tree = tree,
    n.iter = 1000,
    quiet = TRUE
)

# The model recovers the effect of the threshold
# Parameter: "beta_Mating_Age_gt_5"
print(head(fit_logic$summary$statistics))
```

## Example 3: Multi-Class Definitions ("The Rhino Life Cycle")

What if you have multiple life stages?
*   **Newborn**: Age = 0
*   **Juvenile**: Age 1
*   **Subadult**: Age 2-4
*   **Adult**: Age >= 5

You can define this using a sum of logical conditions:

```{r ex3}
# Define 4-level class: 1=Newborn, 2=Juv, 3=Sub, 4=Adult
# IMPORTANT: Data must be INTEGERS matching your formula (1, 2, 3, 4), not strings.
# 'Real' Ages are often integers (years), but the process is continuous.
Age <- round(runif(N, 0, 10), 0)

AgeClass <- 1 * (Age == 0) +
    2 * (Age > 0 & Age < 2) +
    3 * (Age >= 2 & Age < 5) +
    4 * (Age >= 5)

# Outcome: Social Rank increases with Life Stage
Rank <- 1.5 * AgeClass + rnorm(N, sd = 0.5)

# Introduce Missing Data in Age
# Introduce Missing Data in Age
# AgeClass acts as a rigid constraint for imputation.
# Since AgeClass is observed, it 'blocks' information from Rank (d-separation).
# The model imputes Age primarily to satisfy the deterministic condition (e.g. valid age for the class).
# Note: Input ages are integers, but the model imputes continuous values (e.g. 3.5 years).
Age[1:5] <- NA

list_multi <- list(Rank = Rank, Age = Age, AgeClass = AgeClass)

fit_multi <- because(
    equations = list(
        # 1. Link Age to AgeClass
        # This tells the model: "AgeClass" isn't random; it depends on Age phases.
        AgeClass ~ I(
            1 * (Age < 1) +
                2 * (Age >= 1 & Age < 2) +
                3 * (Age >= 2 & Age < 5) +
                4 * (Age >= 5)
        ),

        # 2. Link AgeClass to Rank (Causal: Class -> Rank)
        Rank ~ AgeClass
    ),
    data = list_multi,
    tree = tree,
    n.iter = 1000,
    monitor = "all",
    quiet = TRUE
)

# 1. Check effect of AgeClass on Rank
# Note: Since monitor="all" includes Age[1]..Age[N], the default summary is huge!
# We filter for 'beta' to see the regression coefficients.
stats <- fit_multi$summary$statistics
print(stats[grep("beta", rownames(stats)), ])

# 2. Inspect the imputed ages
# Notice how the model guesses Age based on the observed Rank!
imputed_ages <- extract_imputed(fit_multi)
print(head(imputed_ages))
```


## Example 4: Math (`log(A)`)

You can also use mathematical transformations directly in the formula.

```{r ex3}
Mass <- runif(N, 1, 100) # Ensure positive for log
Metabolism <- 0.75 * log(Mass) + rnorm(N, sd = 0.1)

list_math <- list(Metabolism = Metabolism, Mass = Mass)

fit_math <- because(
    equations = list(
        Metabolism ~ log(Mass)
    ),
    data = list_math,
    tree = tree,
    n.iter = 1000,
    quiet = TRUE
)

# Parameter: "beta_Metabolism_log_Mass"
print(head(fit_math$summary$statistics))
```

## When do I need `I()`?

You might notice we used `I()` for logic but not for math. The rule is:

1.  **Standard Functions**: You interpret things like `log(x)`, `sqrt(x)`, `exp(x)` directly as transformations. No `I()` needed.
    *   `y ~ log(x)` (OK)
2.  **Interactions**: Use `*` or `:`.
    *   `y ~ A * B` (OK)
3.  **Complex Operators**: If you use operators like `>`, `<`, `+`, `-` inside a term, R gets confused. Use `I()` to wrap them.
    *   `y ~ A + B` (Means "A and B are predictors")
    *   `y ~ I(A + B)` (Means "The SUM of A and B is the predictor")
    *   `y ~ I(A > 5)` (Means "True/False is the predictor")


## Why "Deterministic" Matters (The Imputation Advantage)

You might ask: *"Why not just create a column `IsMature` in Excel before loading the data?"*

If your data is complete (no NAs), that works fine. But if you have **missing data**, pre-calculation is dangerous.

### Scenario: The "Broken Logic" of Standard Imputation

Imagine you have a missing `Age` value for a rhino. You want to impute it.

1.  **Standard Approach (e.g., MICE)**:
    *   The imputer sees two separate columns: `Age` (NA) and `IsMature` (NA).
    *   It guesses `Age = 6` based on body size.
    *   It independently guesses `IsMature = 0` based on hormone levels.
    *   **Result**: A rhino that is 6 years old but "Immature". **Impossible!**

2.  **The `because` Approach**:
    *   `IsMature` is not a variable you impute; it is a **formula**.
    *   In MCMC Step 1, the model guesses `Age = 4`. It automatically calculates `IsMature = 0`.
    *   In MCMC Step 2, the model guesses `Age = 6`. It automatically calculates `IsMature = 1`.
    *   **Result**: Every single sample in your posterior distribution is logically consistent. The "physics" of your model are preserved.

### Bonus: Counterfactuals

This also allows for powerful "What If?" questions.
If you want to estimate the effect of aging, you can mathematically intervene on `Age` (set `Age = 8`) and the model knows exactly how that ripples through to `IsMature` and then to `Mating`, without you needing to manually update the downstream variables.
